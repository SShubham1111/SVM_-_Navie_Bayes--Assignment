{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoritical Questions\n",
        "\n",
        "1. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "- A supervised learning algorithm used for classification and regression.\n",
        "- Finds the optimal hyperplane that best separates classes with maximum margin.\n",
        "- Uses support vectors (critical data points) to define the decision boundary.\n",
        "- Works well in high-dimensional spaces.\n",
        "\n",
        "2. Hard Margin vs Soft Margin\n",
        "- Hard Margin\n",
        "  - Assumes data is perfectly linearly separable.\n",
        "  - No misclassification allowed.\n",
        "  - Risk of overfitting and sensitive to noise/outliers.\n",
        "- Soft Margin\n",
        "  - Allows some misclassification using slack variables.\n",
        "  - Balances margin maximization & error minimization.\n",
        "  - Controlled by the penalty parameter C.\n",
        "\n",
        "3. Mathematical intuition behind SVM\n",
        "- Maximize the margin (distance between hyperplane & closest points).\n",
        "- Converts problem into a convex optimization task with constraints.\n",
        "- Uses dot products to measure similarity between samples.\n",
        "- Optimization ensures a unique global minimum.\n",
        "\n",
        "4. Role of Lagrange Multipliers in SVM\n",
        "- Transform constrained optimization → unconstrained dual problem.\n",
        "- Help incorporate margin constraints into the objective function.\n",
        "- Allow kernel trick by expressing solution in dot-product form.\n",
        "- Non-zero multipliers correspond to support vectors.\n",
        "\n",
        "5. What are Support Vectors?\n",
        "- Data points closest to the decision boundary.\n",
        "- Crucial in defining the optimal hyperplane.\n",
        "- Only these points influence the model; others are irrelevant.\n",
        "- Removing them changes the decision boundary.\n",
        "\n",
        "6. What is a Support Vector Classifier (SVC)?\n",
        "- SVM applied to classification problems.\n",
        "- Finds a maximum-margin hyperplane separating classes.\n",
        "- Supports linear and kernel-based separation.\n",
        "- Tuned using C and kernel parameters.\n",
        "\n",
        "7. What is a Support Vector Regressor (SVR)?\n",
        "- SVM adapted for regression tasks.\n",
        "- Uses an ε-insensitive tube to ignore small errors.\n",
        "- Finds a function that fits within max deviation ε.\n",
        "- Controlled by parameters C, ε, and kernel choice.\n",
        "\n",
        "8. What is the Kernel Trick?\n",
        "- Maps data to a higher-dimensional space without explicit transformation.\n",
        "- Computes dot-products using kernel functions instead of coordinates.\n",
        "- Enables SVM to learn non-linear boundaries.\n",
        "- Common kernels: Linear, Polynomial, RBF.\n",
        "\n",
        "9. Compare Linear, Polynomial, and RBF Kernel\n",
        "- Linear\n",
        "  - For linearly separable data.\n",
        "  - Fastest and simplest.\n",
        "  - Good for high-dimensional sparse data (e.g., text).\n",
        "- Polynomial\n",
        "  - Captures interaction features (degree controls complexity).\n",
        "  - Useful for moderately non-linear patterns.\n",
        "  - More expensive than linear.\n",
        "- RBF (Gaussian)\n",
        "  - Handles highly non-linear data.\n",
        "  - Uses distance-based similarity.\n",
        "  - Most widely used; flexible but can overfit.\n",
        "\n",
        "10. Effect of the C parameter in SVM\n",
        "- Controls trade-off between margin width & classification error.\n",
        "- High C → low tolerance for misclassification (hard margin).\n",
        "- Low C → more tolerance (soft margin, smoother boundary).\n",
        "- Affects model complexity & generalization.\n",
        "\n",
        "11. Role of Gamma in RBF Kernel SVM\n",
        "- Controls how far influence of one training example reaches.\n",
        "- High gamma → very narrow influence → risk of overfitting.\n",
        "- Low gamma → wider influence → smoother decision boundary.\n",
        "- Defines curvature of the separating boundary.\n",
        "\n",
        "12. What is Naïve Bayes? Why is it “Naive”?\n",
        "- A probabilistic classifier based on Bayes’ theorem.\n",
        "- Predicts class with highest posterior probability.\n",
        "- “Naïve” because it assumes all features are independent.\n",
        "- Despite assumption, works well in real-world tasks.\n",
        "\n",
        "13. What is Bayes’ Theorem?\n",
        "- Formula to compute posterior probability from prior probability.\n",
        "- Connects likelihood, prior, and evidence.\n",
        "- Foundation for Bayesian classification.\n",
        "\n",
        "14. Differences among Gaussian, Multinomial, Bernoulli Naive Bayes\n",
        "- Gaussian NB\n",
        "  - Assumes continuous features follow a normal distribution.\n",
        "  - Used for numeric data.\n",
        "- Multinomial NB\n",
        "  - Used for discrete count data (e.g., word frequencies).\n",
        "  - Best for NLP tasks like document classification.\n",
        "- Bernoulli NB\n",
        "  - Binary features (0/1: word present or not).\n",
        "  - Works well in text classification with binary representation.\n",
        "\n",
        "15. When to use Gaussian Naive Bayes\n",
        "- When features are continuous (age, height, income).\n",
        "- Data roughly follows a bell-curve distribution.\n",
        "- Not suitable for word counts or binary text data.\n",
        "- Works well even with small sample sizes.\n",
        "\n",
        "16. Key assumptions of Naïve Bayes\n",
        "- Features are conditionally independent given the class.\n",
        "- All features contribute equally and independently.\n",
        "- No correlations between inputs.\n",
        "- Data follows the assumed distribution (Gaussian/Multinomial/Bernoulli).\n",
        "\n",
        "17. Advantages & Disadvantages of Naïve Bayes\n",
        "- Advantages\n",
        "  - Fast, simple, and scalable.\n",
        "  - Works well with small data and high-dimensional data.\n",
        "  - Excellent performance in text classification.\n",
        "- Disadvantages\n",
        "  - Feature independence assumption rarely holds.\n",
        "  - Poor with correlated features.\n",
        "  - Zero-frequency issues (handled by Laplace smoothing).\n",
        "\n",
        "18. Why is Naive Bayes good for text classification?\n",
        "- Words occur independently → matches NB assumption well.\n",
        "- Handles high-dimensional sparse data efficiently.\n",
        "- Performs well even with small training data.\n",
        "- Fast training and prediction.\n",
        "\n",
        "19. Compare SVM and Naive Bayes\n",
        "- SVM\n",
        "  - Margin-based deterministic model.\n",
        "  - Works well with complex boundaries & smaller datasets.\n",
        "  - Needs tuning (C, gamma, kernel).\n",
        "\n",
        "- Naive Bayes\n",
        "  - Probabilistic model with strong independence assumptions.\n",
        "  - Very fast and scalable; best for text data.\n",
        "  - Performs poorly with correlated features.\n",
        "\n",
        "20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "- Adds +1 to word counts to avoid zero probabilities.\n",
        "- Prevents model from assigning zero likelihood to unseen features.\n",
        "- Improves generalization for rare or missing words.\n",
        "- Especially useful in text classification.\n"
      ],
      "metadata": {
        "id": "f8dSRAEVqzO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0_owYL0qyJT"
      },
      "outputs": [],
      "source": []
    }
  ]
}